
# Designing a personalized ChatBot Assistant
# Used WordNetLemmatizer to normalize the lemmatized sentence and word tokens trained on a sample corpus
# Used TF-IDF vectorizer to tokenize documents, learn the vocabulary and inverse frequency document weightings
# Trained a GPT-2 Transformer Encoder model with 124M parameters for sentiment analysis and intent classification on the corpus
